{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 7: Transfer Learning and Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Transfer learning contexts\n",
    "- Leveraging pre-trained models for supervised domain adaptation\n",
    "- Unsupervised domain adaptation using adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:34.467438Z",
     "iopub.status.busy": "2020-11-24T17:31:34.466734Z",
     "iopub.status.idle": "2020-11-24T17:31:35.132146Z",
     "shell.execute_reply": "2020-11-24T17:31:35.132546Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.135780Z",
     "iopub.status.busy": "2020-11-24T17:31:35.135305Z",
     "iopub.status.idle": "2020-11-24T17:31:35.155762Z",
     "shell.execute_reply": "2020-11-24T17:31:35.156319Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The supervised learning context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We have a labeled dataset of $N$ labelled samples: $\\left\\{ (\\vec{x}^i,y^i) \\right\\}_{i=1}^N$, where\n",
    "- $\\vec{x}^i = \\left(x^i_1, \\dots, x^i_D\\right) \\in \\mathcal{X}$  is a **sample** or **feature vector**.\n",
    "- $y^i \\in \\mathcal{Y}$ is the **label**.\n",
    "- For classification with $C$ classes, $\\mathcal{Y} = \\{0,\\dots,C-1\\}$, so each $y^i$ is a **class label**.\n",
    "- Usually we assume each labeled sample $(\\vec{x}^i,y^i)$\n",
    "  is drawn from a joint distribution\n",
    "  $$P(\\rvec{X}, \\rvar{Y})=P(\\rvec{X})\\cdot P(\\rvar{Y}|\\rvec{X})$$\n",
    "    - We assume some marginal sample distribution $P(\\rvar{X})$ exists.\n",
    "    - We want to learn $P(\\rvar{Y}|\\rvec{X})$ from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far, we considered mostly the traditional **supervised learning** setting:\n",
    "\n",
    "We assumed the **train** and **test** (which is supposed to represent future unseen data)\n",
    "sets are both from the same **distribution** $P(\\rvec{X}, \\rvar{Y})=P(\\rvec{X})\\cdot P(\\rvar{Y}|\\rvec{X})$ and both labeled.\n",
    "\n",
    "We were able to assume this since we wanted to solve one task with one dataset, and we could\n",
    "therefore split our dataset into such sets.\n",
    "\n",
    "What happens when this is not the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the real world, we often don't have the perfect training set for our problem.\n",
    "\n",
    "What should we do when the supervised learning assumption is invalid?\n",
    "\n",
    "<center><img src=\"img/transfer_learning_digits.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Domains, targets and tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets start with some definitions to explain the problem.\n",
    "\n",
    "- Imagine we have a **feature space**, $\\mathcal{X}$\n",
    "    - For example, $\\mathcal{X}$ is the space of color images of size 32x32, each pixel in the range 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.159670Z",
     "iopub.status.busy": "2020-11-24T17:31:35.159177Z",
     "iopub.status.idle": "2020-11-24T17:31:35.179438Z",
     "shell.execute_reply": "2020-11-24T17:31:35.179981Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10^7398\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# size of this \"limited\" feature space\n",
    "print(f'10^{math.log10(256**(32**2*3)):.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- As usual, we have a training set $X=\\{\\vec{x}^{(i)}\\}_{i=1}^{N},\\ \\vec{x}^{(i)}\\in\\cset{X}$.\n",
    "    - For example, CIFAR-10\n",
    "    \n",
    "<center><img src=\"img/cifar10.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There exists some **probability distribution** $P(X)$ (aka $P_{X}(\\vec{x})$) over our data.\n",
    "    - Note that we don't care about the distribution over $\\cset{X}$.\n",
    "    - For example, if $X$ is CIFAR-10, the probability of an all-black image should be very low\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Our **label space**, $\\cset{Y}$ includes the possible labels for sample in our problem.\n",
    "    - For example $\\cset{Y}=\\{0,1\\}$ in binary classification.\n",
    "- We may have also $Y = \\{y^{(i)}\\}_{i=1}^{N},\\ y^{(i)}\\in\\cset{Y}$, the set of labels for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to learn the target function $\\hat{y}=f(\\vec{x})$ which predicts a label given an image.\n",
    "    - From the probabilistic perspective, learn $P(\\hat{y}|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally,\n",
    "- A learning **domain** $\\cset{D}$, is defined as $\\cset{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "- A learning **task** $\\cset{T}$ is defined as $\\cset{T}=\\{\\cset{Y},P(Y|X)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer learning settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition** (Pan & Yang, 2010):\n",
    "\n",
    "Given\n",
    "- A **source** domain $\\cset{D}_S=\\left\\{\\mathcal{X}_S,P(X_S)\\right\\}$ and source learning task\n",
    "  $\\cset{T}_S = \\{\\cset{Y}_S,P(Y_S|X_S)\\}$\n",
    "  \n",
    "- A **target** domain $\\cset{D}_T=\\left\\{\\mathcal{X}_T,P(X_T)\\right\\}$ and target learning task\n",
    "  $\\cset{T}_T = \\{\\cset{Y}_T,P(Y_T|X_T)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Transfer learning* deals with learning of the target function $P(Y_T|X_T)$\n",
    "using *knowledge* of $\\cset{D}_S$ and $\\cset{T}_S$, when\n",
    "- $\\cset{D}_S \\neq \\cset{D}_T$, or\n",
    "- $\\cset{T}_S \\neq \\cset{T}_T$\n",
    "\n",
    "Usually also there are other constraints on the target domain, such as little or no labels available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $\\cset{D}_S=\\cset{D}_T$ and $\\cset{T}_S=\\cset{T}_T$ we're in the regular supervised learning setting\n",
    "we have seen thus far.\n",
    "For example, splitting CIFAR-10 randomly into a train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Same domain, different task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall, a learning **task** $\\cset{T}$ is defined as $\\cset{T}=\\{\\cset{Y},P(Y|X)\\}$.\n",
    "\n",
    "So there are two cases (not mutually exclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case 1: The label spaces are different, $\\cset{Y}_S \\neq \\cset{Y}_T$\n",
    "\n",
    "For example, target domain has more classes.\n",
    "\n",
    "<center><img src=\"img/cifar10_100.png\" width=\"1100\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case 2: The target conditional distributions are different, $P(Y_S|X_S)\\neq P(Y_T|X_S)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This may be the case when the class-balance is very different in the source and target distributions, i.e.\n",
    "we have a different prior $P(Y)$ for the labels between source and target.\n",
    "\n",
    "<center><img src=\"img/data_dist.jpg\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Same task, different domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall, a learning **domain** $\\cset{D}$, is defined as $\\cset{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "\n",
    "Again, two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Case 1: Different feature spaces, $\\cset{X}_S \\neq \\cset{X}_T$.\n",
    "\n",
    "For example:\n",
    "- $\\cset{X}_S$ is a space of grayscale images while $\\cset{X}_T$ is a space of color images\n",
    "- Documents in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case 2: Different data (evidence) distributions, $P(X_S)\\neq P(X_T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example:\n",
    "- Source domain contains hand-drawn images, while target domain contains photographs;\n",
    "- Documents in the same language about different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/tl_example.png\" width=\"500\"/></center>\n",
    "\n",
    "This is a very common scenario, and usually called **domain adaptation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transfer learning is a huge research field.\n",
    "\n",
    "<center><img src=\"img/pan_yang.png\" width=\"1100\" /></center>\n",
    "\n",
    "In this tutorial we'll see two simple yet common examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Fine-tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have trained trained a model in a source domain,\n",
    "and now we want to use it to speed up training for a different domain.\n",
    "\n",
    "In some applications, we may have have much less labeled data in the target domain, making it infeasible to train a deep model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Common example: pre-train on ImageNet (1M+ images, 1000 classes), and then classify e.g. medical images.\n",
    "\n",
    "<center><img src=\"img/transfer-learning-medical.png\" width=\"650\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why would this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs capture hierarchical features, with deeper layers capturing higher-level, class-specific features.\n",
    "\n",
    "<center><img src=\"img/cnn_feature_vis.png\" width=\"1700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are we looking at? Images generated by optimization to maximally activate various layers (aka. DeepDream objective) of a GoogLeNet trained on the ImageNet data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "General idea: we can start from a pre-trained model and,\n",
    "- Keep the parameters in the base layer as-is.\n",
    "- \"Fine-tune\" the convolutional filters, mainly in the deeper layers.\n",
    "- Change the classifier head (or completely remove it) to fit our task and train it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As an example,\n",
    "- We'll load a deep CNN pre-trained on ImageNet (1000 classes, 1M+ 224x224 images)\n",
    "- Using ResNet18 just to reduce download size, you can use something deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.184566Z",
     "iopub.status.busy": "2020-11-24T17:31:35.184081Z",
     "iopub.status.idle": "2020-11-24T17:31:35.564417Z",
     "shell.execute_reply": "2020-11-24T17:31:35.565005Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision as tv\n",
    "\n",
    "resnet18 = tv.models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can see the expected structure of the ResNet: input layer (1 conv), then 4 \"layers\" with 2 ResNet blocks each (16 convs) and an output classification layer.\n",
    "- We can also see 1000 output classes on the final FC layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, lets **freeze** all layers: Disable gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.568823Z",
     "iopub.status.busy": "2020-11-24T17:31:35.568191Z",
     "iopub.status.idle": "2020-11-24T17:31:35.590531Z",
     "shell.execute_reply": "2020-11-24T17:31:35.591121Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll decide to fine-tune only the convolutions in layer 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.594319Z",
     "iopub.status.busy": "2020-11-24T17:31:35.593802Z",
     "iopub.status.idle": "2020-11-24T17:31:35.616440Z",
     "shell.execute_reply": "2020-11-24T17:31:35.616832Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# \"Thaw\" last layer (or whatever is relevant for you)\n",
    "for p in resnet18.layer4.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A different approach to freezing: Set learning rates to zero per-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.620845Z",
     "iopub.status.busy": "2020-11-24T17:31:35.620342Z",
     "iopub.status.idle": "2020-11-24T17:31:35.642823Z",
     "shell.execute_reply": "2020-11-24T17:31:35.643398Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "# Another way to freeze: zero learning rates for specific parameters\n",
    "opt = torch.optim.SGD([\n",
    "    dict(params=resnet18.layer1.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer2.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer3.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer4.parameters(), lr=1e-4),\n",
    "    dict(params=resnet18.fc.parameters()),\n",
    "], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we need to replace the fully-connected part by some other classifier, which fits our target task, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.647157Z",
     "iopub.status.busy": "2020-11-24T17:31:35.646654Z",
     "iopub.status.idle": "2020-11-24T17:31:35.671186Z",
     "shell.execute_reply": "2020-11-24T17:31:35.670760Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_features=512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Number of conv features coming into the FC\n",
    "cnn_features = resnet18.fc.in_features\n",
    "print(f'cnn_features={cnn_features}')\n",
    "\n",
    "# Number of classes in our target task\n",
    "num_classes = 13\n",
    "\n",
    "resnet18.fc =  nn.Sequential(\n",
    "    nn.Linear(cnn_features, 100, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_classes, bias=True),\n",
    ")\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use CIFAR-10 as an example target domain and task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:35.675689Z",
     "iopub.status.busy": "2020-11-24T17:31:35.675199Z",
     "iopub.status.idle": "2020-11-24T17:31:37.207085Z",
     "shell.execute_reply": "2020-11-24T17:31:37.207768Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "# Important nuance 1: need to resize and scale our data same as ImageNet training data\n",
    "tf = tvtf.Compose([\n",
    "    tvtf.Resize(224),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load our target domain data (CIFAR-10 used just as a simple example)\n",
    "ds_train = tv.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tf)\n",
    "ds_test = tv.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tf)\n",
    "\n",
    "batch_size = 8\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=2)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Feed our modified ResNet a CIFAR-10 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:37.212006Z",
     "iopub.status.busy": "2020-11-24T17:31:37.211063Z",
     "iopub.status.idle": "2020-11-24T17:31:37.352558Z",
     "shell.execute_reply": "2020-11-24T17:31:37.353259Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0049,  0.0693,  0.0710, -0.0467,  0.1250,  0.2077, -0.0535,  0.1748,\n",
      "         -0.1400, -0.1524,  0.2084, -0.1545,  0.4580]],\n",
      "       grad_fn=<AddmmBackward>) torch.Size([1, 13])\n"
     ]
    }
   ],
   "source": [
    "y0 = resnet18(ds_train[0][0].unsqueeze(dim=0))\n",
    "print(y0, y0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Set up optimization to account for the fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:37.357350Z",
     "iopub.status.busy": "2020-11-24T17:31:37.356585Z",
     "iopub.status.idle": "2020-11-24T17:31:37.380251Z",
     "shell.execute_reply": "2020-11-24T17:31:37.380792Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Important nunance 2: Only parameters that track gradients can be passed into the optimizer\n",
    "params_non_frozen = filter(lambda p: p.requires_grad, resnet18.parameters())\n",
    "opt = optim.SGD(params_non_frozen, lr=0.05, momentum=0.9)\n",
    "\n",
    "# Finetuning usually means we want smaller than usual learning rates and \n",
    "# decaying them in order to keep improving the weights\n",
    "lr_sched = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.05, patience=5,)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, train as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:37.383960Z",
     "iopub.status.busy": "2020-11-24T17:31:37.383381Z",
     "iopub.status.idle": "2020-11-24T17:31:37.404660Z",
     "shell.execute_reply": "2020-11-24T17:31:37.405190Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, lr_sched, dl_train, dl_test):\n",
    "    # Same as regular classifier traning, just call lr_sched.step() every epoch.\n",
    "    # ...\n",
    "    # ====== YOUR CODE: ======\n",
    "    # :)\n",
    "    # ========================\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Unsupervised domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider a problem with different domains but an identical task:\n",
    "\n",
    "- Source domain: MNIST\n",
    "- Target domain: MNIST-M, a colored and textured version of MNIST\n",
    "\n",
    "Task in both cases is the usual 10-class digit classification.\n",
    "\n",
    "<img src=\"img/mnist_m.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Unsupervised** DA setting:\n",
    "We assume that there are **no available labels** for the target domain.\n",
    "\n",
    "Why would a CNN trained on MNIST not generalize to MNIST-M?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuition: We need a way to force our CNN to learn features of the digit **shapes** only, not color distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our approach (based on Ganin et al. 2015): \"Domain-adversarial\" training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/ganin_da.png\" width=\"1700\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Train a classifier for the **label** of images from the **source** domain as a regular CNN feature extractor and FC classifier.\n",
    "- Train a classifier for the **domain** of an image based on the deep convolutonal features.\n",
    "- Try to **maximize** the loss of this domain classifier when training the convolutional layers (**confusion loss**).\n",
    "- Simultaneously, minimize the classification loss on the source domain using the same convolutional features.\n",
    "- Train the digit classifier with source domain data, and the domain classifier with both domains' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Source and target domain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: for the next block to run, you should manually [download](https://drive.google.com/open?id=0B_tExHiYS-0veklUZHFYT19KYjg) the MNIST-M dataset and unpack it into `data_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:31:37.411031Z",
     "iopub.status.busy": "2020-11-24T17:31:37.410281Z",
     "iopub.status.idle": "2020-11-24T17:32:13.917256Z",
     "shell.execute_reply": "2020-11-24T17:32:13.916865Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 576x576 with 3 Axes>,\n",
       " array([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>], dtype=object))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tut7.data import MNISTMDataset\n",
    "from tut7.plot_utils import dataset_first_n\n",
    "\n",
    "image_size = 28\n",
    "batch_size = 4\n",
    "tf_source = tvtf.Compose([ tvtf.Resize(image_size), tvtf.ToTensor(), tvtf.Normalize(mean=(0.1307,), std=(0.3081,)) ])\n",
    "tf_target = tvtf.Compose([\n",
    "    tvtf.Resize(image_size), tvtf.ToTensor(), tvtf.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "ds_source = tv.datasets.MNIST(root=data_dir, train=True, transform=tf_source, download=True)\n",
    "# Custom PyTorch Dataset class to load MNIST-M\n",
    "ds_target = MNISTMDataset(os.path.join(data_dir, 'mnist_m', 'mnist_m_train'),\n",
    "                          os.path.join(data_dir, 'mnist_m', 'mnist_m_train_labels.txt'),\n",
    "                         transform=tf_target)\n",
    "\n",
    "dataset_first_n(ds_source, 3, cmap='gray');\n",
    "dataset_first_n(ds_target, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:13.920813Z",
     "iopub.status.busy": "2020-11-24T17:32:13.920308Z",
     "iopub.status.idle": "2020-11-24T17:32:13.945488Z",
     "shell.execute_reply": "2020-11-24T17:32:13.946114Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our model will consist of three parts, as in the figure:\n",
    "- A \"deep\" CNN for image feature extraction (2x Conv, ReLU, MaxPool)\n",
    "- A digit-classification head (3x FC, ReLU)\n",
    "- A domain classification head (2x FC, ReLU), with **gradient reversal layer** (GRL).\n",
    "\n",
    "\n",
    "<center><img src=\"img/ganin_da2.png\" width=\"1600\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the gradient reversal layer doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "GRL is no-op in forward pass, but applies $-\\lambda$ factor to gradient in the backward pass.\n",
    "\n",
    "How can we implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`autograd.Function` objects are what PyTorch uses to record operation history on tensors.\n",
    "\n",
    "They define the functions used for the forward and backprop of any tensor operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:13.950300Z",
     "iopub.status.busy": "2020-11-24T17:32:13.949796Z",
     "iopub.status.idle": "2020-11-24T17:32:13.973425Z",
     "shell.execute_reply": "2020-11-24T17:32:13.974045Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        # Store context for backprop\n",
    "        ctx.alpha = alpha\n",
    "        \n",
    "        # Forward pass is a no-op\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # grad_output is dL/dx (since our forward's output was x)\n",
    "        \n",
    "        # Backward pass is just to -alpha the gradient\n",
    "        # This will become the new dL/dx in the rest of the network\n",
    "        output =  - ctx.alpha * grad_output\n",
    "\n",
    "        # Must return number of inputs to forward()\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:13.977821Z",
     "iopub.status.busy": "2020-11-24T17:32:13.977314Z",
     "iopub.status.idle": "2020-11-24T17:32:14.001230Z",
     "shell.execute_reply": "2020-11-24T17:32:14.001743Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5., 7., 9.], grad_fn=<GradientReversalFnBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([1,2,3,4.], requires_grad=True)\n",
    "t = 2 * w + 1\n",
    "t = GradientReversalFn.apply(t, 0.25)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.004632Z",
     "iopub.status.busy": "2020-11-24T17:32:14.004147Z",
     "iopub.status.idle": "2020-11-24T17:32:14.028337Z",
     "shell.execute_reply": "2020-11-24T17:32:14.028892Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5000, -0.5000, -0.5000, -0.5000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(t)\n",
    "loss.backward(retain_graph=True) # don't discard computation graph during backward, for later vizualization\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.032424Z",
     "iopub.status.busy": "2020-11-24T17:32:14.031955Z",
     "iopub.status.idle": "2020-11-24T17:32:14.099564Z",
     "shell.execute_reply": "2020-11-24T17:32:14.100113Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"169pt\" height=\"264pt\"\n",
       " viewBox=\"0.00 0.00 169.28 264.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-260 165.28,-260 165.28,4 -4,4\"/>\n",
       "<!-- 140211235979376 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140211235979376</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"126.79,-20 34.49,-20 34.49,0 126.79,0 126.79,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.64\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 140211236012096 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140211236012096</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"161.43,-76 -0.14,-76 -0.14,-56 161.43,-56 161.43,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.64\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\">GradientReversalFnBackward</text>\n",
       "</g>\n",
       "<!-- 140211236012096&#45;&gt;140211235979376 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140211236012096&#45;&gt;140211235979376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.64,-55.59C80.64,-48.7 80.64,-39.1 80.64,-30.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.14,-30.3 80.64,-20.3 77.14,-30.3 84.14,-30.3\"/>\n",
       "</g>\n",
       "<!-- 140211235980816 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140211235980816</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"126.45,-132 34.83,-132 34.83,-112 126.45,-112 126.45,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.64\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140211235980816&#45;&gt;140211236012096 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140211235980816&#45;&gt;140211236012096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.64,-111.59C80.64,-104.7 80.64,-95.1 80.64,-86.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.14,-86.3 80.64,-76.3 77.14,-86.3 84.14,-86.3\"/>\n",
       "</g>\n",
       "<!-- 140211724051936 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140211724051936</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"125.79,-188 35.49,-188 35.49,-168 125.79,-168 125.79,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.64\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140211724051936&#45;&gt;140211235980816 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140211724051936&#45;&gt;140211235980816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.64,-167.59C80.64,-160.7 80.64,-151.1 80.64,-142.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.14,-142.3 80.64,-132.3 77.14,-142.3 84.14,-142.3\"/>\n",
       "</g>\n",
       "<!-- 140211236051024 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140211236051024</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107.64,-256 53.64,-256 53.64,-224 107.64,-224 107.64,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.64\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\">w</text>\n",
       "<text text-anchor=\"middle\" x=\"80.64\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (4)</text>\n",
       "</g>\n",
       "<!-- 140211236051024&#45;&gt;140211724051936 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140211236051024&#45;&gt;140211724051936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.64,-223.86C80.64,-216.13 80.64,-206.63 80.64,-198.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.14,-198.15 80.64,-188.15 77.14,-198.15 84.14,-198.15\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8578ea0490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "torchviz.make_dot(loss, params=dict(w=w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's implement the model exactly as in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.108079Z",
     "iopub.status.busy": "2020-11-24T17:32:14.107495Z",
     "iopub.status.idle": "2020-11-24T17:32:14.137281Z",
     "shell.execute_reply": "2020-11-24T17:32:14.137837Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DACNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, padding=1, stride=1),  # (28+2P-F)/S + 1 = 26\n",
    "            nn.BatchNorm2d(64), nn.MaxPool2d(2), nn.ReLU(True),    # 26 / 2 = 13\n",
    "            nn.Conv2d(64, 50, kernel_size=5, padding=1, stride=1), # (12+2P-F)/S + 1 = 10\n",
    "            nn.BatchNorm2d(50), nn.MaxPool2d(2), nn.ReLU(True),    # 10 / 2 = 5\n",
    "            nn.Dropout2d(), \n",
    "        )\n",
    "        self.num_cnn_features = 50 * 5 * 5\n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_cnn_features, 100),\n",
    "            nn.BatchNorm1d(100), nn.Dropout2d(), nn.ReLU(True),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.BatchNorm1d(100), nn.ReLU(True),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_cnn_features, 100),\n",
    "            nn.BatchNorm1d(100), nn.ReLU(True),\n",
    "            nn.Linear(100, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "    def forward(self, x, grl_lambda=1.0):\n",
    "        # Handle single-channel input by expanding (repeating) the singleton dimention\n",
    "        x = x.expand(x.data.shape[0], 3, image_size, image_size)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(-1, self.num_cnn_features)\n",
    "        features_grl = GradientReversalFn.apply(features, grl_lambda)\n",
    "        class_pred = self.class_classifier(features)        # classify on regular features\n",
    "        domain_pred = self.domain_classifier(features_grl)  # classify on features after GRL\n",
    "        return class_pred, domain_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wait, but why let $\\lambda$ (`grl_lambda` in the code) change during training (e.g. every epoch)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the beginning of training, the domain loss is extremely noisy since the CNN features are not good yet.\n",
    "- We don't want to backprop domain confusion into the CNN layers in the beginning.\n",
    "- Therefore, lambda is gradulaly changed from 0 to 1 in the course of training.\n",
    "    $$\n",
    "    \\lambda_p = \\frac{2}{1+\\exp(-10\\cdot p)} -1,\n",
    "    $$\n",
    "    where $p\\in[0,1]$ is the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.141900Z",
     "iopub.status.busy": "2020-11-24T17:32:14.141390Z",
     "iopub.status.idle": "2020-11-24T17:32:14.190368Z",
     "shell.execute_reply": "2020-11-24T17:32:14.189692Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source domain input:  torch.Size([4, 1, 28, 28]) torch.Size([4])\n",
      "target domain input:  torch.Size([4, 3, 28, 28]) torch.Size([4])\n",
      "yhat0_t_c:\n",
      " tensor([[-2.9556, -2.0405, -2.0426, -2.6291, -2.0487, -1.9392, -2.7778, -2.2558,\n",
      "         -2.3180, -2.5512],\n",
      "        [-2.0808, -3.1958, -2.2480, -2.7248, -1.9556, -2.5797, -1.5869, -2.5399,\n",
      "         -2.2374, -2.8892],\n",
      "        [-1.9534, -2.4835, -1.8684, -2.4796, -2.0358, -3.0086, -2.4753, -2.7293,\n",
      "         -2.3817, -2.1640],\n",
      "        [-2.5958, -2.1273, -2.8562, -2.5320, -1.7657, -2.6998, -2.6604, -2.7680,\n",
      "         -2.1450, -1.7080]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 10])\n",
      "yhat0_t_d:\n",
      " tensor([[-0.3585, -1.1998],\n",
      "        [-0.8676, -0.5447],\n",
      "        [-0.6741, -0.7126],\n",
      "        [-0.8918, -0.5275]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "model = DACNN()\n",
    "\n",
    "x0_s, y0_s = next(iter(dl_source))\n",
    "x0_t, y0_t = next(iter(dl_target))\n",
    "\n",
    "print('source domain input: ', x0_s.shape, y0_s.shape)\n",
    "print('target domain input: ', x0_t.shape, y0_t.shape)\n",
    "\n",
    "# Test forward pass: get class prediction and domain prediction\n",
    "yhat0_s_c, yhat0_s_d = model(x0_s)\n",
    "yhat0_t_c, yhat0_t_d = model(x0_t)\n",
    "\n",
    "print('yhat0_t_c:\\n', yhat0_t_c, yhat0_t_c.shape)\n",
    "print('yhat0_t_d:\\n', yhat0_t_d, yhat0_t_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.194731Z",
     "iopub.status.busy": "2020-11-24T17:32:14.194100Z",
     "iopub.status.idle": "2020-11-24T17:32:14.222839Z",
     "shell.execute_reply": "2020-11-24T17:32:14.223498Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 1\n",
    "\n",
    "# Setup optimizer as usual\n",
    "model = DACNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Two loss functions this time (can generally be different)\n",
    "loss_fn_class = torch.nn.NLLLoss()\n",
    "loss_fn_domain = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.227654Z",
     "iopub.status.busy": "2020-11-24T17:32:14.227035Z",
     "iopub.status.idle": "2020-11-24T17:32:14.252022Z",
     "shell.execute_reply": "2020-11-24T17:32:14.252553Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)\n",
    "\n",
    "# We'll train the same number of batches from both datasets\n",
    "max_batches = min(len(dl_source), len(dl_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T17:32:14.258576Z",
     "iopub.status.busy": "2020-11-24T17:32:14.257823Z",
     "iopub.status.idle": "2020-11-24T17:32:29.895034Z",
     "shell.execute_reply": "2020-11-24T17:32:29.895584Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 / 0001\n",
      "=================\n",
      "[1/231] class_loss: 2.4172 s_domain_loss: 1.1109 t_domain_loss: 0.4515 grl_lambda: 0.000 \n",
      "[2/231] class_loss: 2.2705 s_domain_loss: 1.0442 t_domain_loss: 0.4806 grl_lambda: 0.022 \n",
      "[3/231] class_loss: 2.1664 s_domain_loss: 0.9797 t_domain_loss: 0.4975 grl_lambda: 0.043 \n",
      "[4/231] class_loss: 2.0120 s_domain_loss: 0.9326 t_domain_loss: 0.5375 grl_lambda: 0.065 \n",
      "[5/231] class_loss: 1.9922 s_domain_loss: 0.8885 t_domain_loss: 0.5575 grl_lambda: 0.086 \n",
      "[6/231] class_loss: 1.8970 s_domain_loss: 0.8295 t_domain_loss: 0.5826 grl_lambda: 0.108 \n",
      "[7/231] class_loss: 1.8194 s_domain_loss: 0.7794 t_domain_loss: 0.6207 grl_lambda: 0.129 \n",
      "[8/231] class_loss: 1.7999 s_domain_loss: 0.7407 t_domain_loss: 0.6560 grl_lambda: 0.150 \n",
      "[9/231] class_loss: 1.6827 s_domain_loss: 0.7087 t_domain_loss: 0.6902 grl_lambda: 0.171 \n",
      "[10/231] class_loss: 1.6249 s_domain_loss: 0.6685 t_domain_loss: 0.7058 grl_lambda: 0.192 \n",
      "[11/231] class_loss: 1.5978 s_domain_loss: 0.6444 t_domain_loss: 0.7353 grl_lambda: 0.213 \n",
      "This is just a demo, stopping...\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(n_epochs):\n",
    "    print(f'Epoch {epoch_idx+1:04d} / {n_epochs:04d}', end='\\n=================\\n')\n",
    "    dl_source_iter = iter(dl_source)\n",
    "    dl_target_iter = iter(dl_target)\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        optimizer.zero_grad()\n",
    "        # Training progress and GRL lambda\n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        # === Train on source domain\n",
    "        X_s, y_s = next(dl_source_iter)\n",
    "        y_s_domain = torch.zeros(batch_size, dtype=torch.long) # generate source domain labels: 0\n",
    "\n",
    "        class_pred, domain_pred = model(X_s, grl_lambda)\n",
    "        loss_s_label = loss_fn_class(class_pred, y_s)           # source classification loss\n",
    "        loss_s_domain = loss_fn_domain(domain_pred, y_s_domain) # source domain loss (via GRL)\n",
    "\n",
    "        # === Train on target domain\n",
    "        X_t, _ = next(dl_target_iter) # Note: ignoring target domain class labels!\n",
    "        y_t_domain = torch.ones(batch_size, dtype=torch.long) # generate target domain labels: 1\n",
    "\n",
    "        _, domain_pred = model(X_t, grl_lambda)\n",
    "        loss_t_domain = loss_fn_domain(domain_pred, y_t_domain) # target domain loss (via GRL)\n",
    "        \n",
    "        # === Optimize\n",
    "        loss = loss_t_domain + loss_s_domain + loss_s_label\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'[{batch_idx+1}/{max_batches}] '\n",
    "              f'class_loss: {loss_s_label.item():.4f} ' f's_domain_loss: {loss_s_domain.item():.4f} '\n",
    "              f't_domain_loss: {loss_t_domain.item():.4f} ' f'grl_lambda: {grl_lambda:.3f} '\n",
    "             )\n",
    "        if batch_idx == 10:\n",
    "            print('This is just a demo, stopping...')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embeddings visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's useful to visualize the space of the convolutional features learned by the model.\n",
    "\n",
    "Recall, our domain confusion loss was supposed to make images from both domains look the same for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/ganin_da3.png\" width=\"1400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows t-SNE visualizations of the CNN’s activations (a) in case when no adaptation was performed and (b) in case when our adaptation procedure was incorporated into training. Blue points correspond to the source domain examples, while red ones correspond to the target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Pan & Yang, 2010, A Survey on Transfer Learning\n",
    "- C. Olah et al. 2017, Feature Visualization\n",
    "- Y. Ganin et al. 2015, Domain-Adversarial Training of Neural Networks \n",
    "- M. Wulfmeier et al., https://arxiv.org/abs/1703.01461v2\n",
    "- Sebastian Ruder, http://ruder.io/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
